"""
NewsQuant ê°ì„±ë¶„ì„ ì§„ë‹¨ ìŠ¤í¬ë¦½íŠ¸
================================
í˜„ì¬ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„±ë¶„ì„ì˜ ë¬¸ì œì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì§„ë‹¨í•©ë‹ˆë‹¤.

ì‹¤í–‰: python scripts/diagnose_sentiment.py
"""

import sys
import os
import sqlite3
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

DB_PATH = PROJECT_ROOT / "news_data.db"


def load_news_df():
    """DBì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
    conn = sqlite3.connect(str(DB_PATH))
    df = pd.read_sql_query("""
        SELECT id, title, content, published_at, source, category,
               related_stocks, sentiment_score, overall_score,
               importance_score, impact_score, timeliness_score
        FROM news
    """, conn)
    conn.close()
    df['published_date'] = pd.to_datetime(df['published_at'].str[:10], errors='coerce')
    return df


def diagnose_sentiment_distribution(df):
    """Phase 1-1: sentiment_score ë¶„í¬ ë¶„ì„"""
    print("=" * 70)
    print("1. SENTIMENT SCORE ë¶„í¬ ë¶„ì„")
    print("=" * 70)

    total = len(df)
    zero = (df['sentiment_score'] == 0).sum()
    positive = (df['sentiment_score'] > 0).sum()
    negative = (df['sentiment_score'] < 0).sum()
    null_count = df['sentiment_score'].isna().sum()

    print(f"  ì „ì²´ ë‰´ìŠ¤: {total:,}ê±´")
    print(f"  ì¤‘ë¦½ (score=0): {zero:,}ê±´ ({zero/total*100:.1f}%)")
    print(f"  ê¸ì • (score>0): {positive:,}ê±´ ({positive/total*100:.1f}%)")
    print(f"  ë¶€ì • (score<0): {negative:,}ê±´ ({negative/total*100:.1f}%)")
    print(f"  NULL: {null_count:,}ê±´")
    print(f"  í‰ê· : {df['sentiment_score'].mean():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {df['sentiment_score'].std():.4f}")
    print()

    # íˆìŠ¤í† ê·¸ë¨ í…ìŠ¤íŠ¸
    bins = np.arange(-1.05, 1.15, 0.1)
    hist, edges = np.histogram(df['sentiment_score'].dropna(), bins=bins)
    print("  ë¶„í¬ íˆìŠ¤í† ê·¸ë¨:")
    max_bar = max(hist) if max(hist) > 0 else 1
    for i in range(len(hist)):
        label = f"  [{edges[i]:+.1f}, {edges[i+1]:+.1f})"
        bar = "â–ˆ" * int(hist[i] / max_bar * 40)
        print(f"  {label} {bar} {hist[i]:>6,}")
    print()

    # ë¬¸ì œì  1: ì¤‘ë¦½ ë¹„ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ
    print(f"  âš ï¸ ì§„ë‹¨: ì¤‘ë¦½ ë¹„ìœ¨ {zero/total*100:.0f}%ë¡œ ê°ì„±ë¶„ì„ì˜ ë³€ë³„ë ¥ ë¶€ì¡±")
    print(f"  âš ï¸ ì§„ë‹¨: í‰ê·  {df['sentiment_score'].mean():.4f}ë¡œ ì•½ê°„ ê¸ì • í¸í–¥")
    return {'zero_ratio': zero/total, 'pos_ratio': positive/total, 'neg_ratio': negative/total}


def diagnose_stock_sentiment_vs_return(df):
    """Phase 1-2: sentiment_scoreì™€ ì£¼ê°€ ìˆ˜ìµë¥  ìƒê´€ê´€ê³„"""
    print("=" * 70)
    print("2. SENTIMENT vs ì£¼ê°€ ìˆ˜ìµë¥  ìƒê´€ê´€ê³„")
    print("=" * 70)

    # ìœ íš¨í•œ ì¢…ëª©ì½”ë“œê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°
    stock_news = df[df['related_stocks'].str.match(r'^\d{6}$', na=False)].copy()
    print(f"  ìœ íš¨ ì¢…ëª©ì½”ë“œ ë‰´ìŠ¤: {len(stock_news):,}ê±´")

    # ì¢…ëª©-ì¼ìë³„ ì§‘ê³„
    stock_daily = stock_news.groupby(
        [stock_news['related_stocks'], stock_news['published_date']]
    ).agg(
        avg_sentiment=('sentiment_score', 'mean'),
        news_count=('id', 'count'),
        max_sentiment=('sentiment_score', 'max'),
        min_sentiment=('sentiment_score', 'min'),
    ).reset_index()

    # ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° (ë„¤ì´ë²„ ê¸ˆìœµ)
    print("\n  ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘ (ìƒìœ„ ì¢…ëª©)...")
    from news_scraper.price_fetcher import PriceFetcher
    import time

    fetcher = PriceFetcher()
    top_stocks = stock_news['related_stocks'].value_counts().head(15).index.tolist()

    results = []
    for code in top_stocks:
        try:
            price_df = fetcher.get_daily_price(code, pages=10)
            if price_df.empty:
                continue
            price_df = price_df.sort_values('ë‚ ì§œ').reset_index(drop=True)
            price_df['ë‚ ì§œ'] = pd.to_datetime(price_df['ë‚ ì§œ'])
            price_df['ì¢…ê°€'] = pd.to_numeric(price_df['ì¢…ê°€'].astype(str).str.replace(',', ''), errors='coerce')

            # ê° ë‰´ìŠ¤ ë‚ ì§œì— ëŒ€í•´ ì´í›„ 5/10/15ì¼ ìˆ˜ìµë¥  ê³„ì‚°
            code_news = stock_daily[stock_daily['related_stocks'] == code]
            for _, row in code_news.iterrows():
                news_date = row['published_date']
                # ë‰´ìŠ¤ ë‚ ì§œ ì´í›„ ê°€ê²© ì°¾ê¸°
                future = price_df[price_df['ë‚ ì§œ'] > news_date].sort_values('ë‚ ì§œ')
                past = price_df[price_df['ë‚ ì§œ'] <= news_date].sort_values('ë‚ ì§œ')

                if past.empty or future.empty:
                    continue

                base_price = past.iloc[-1]['ì¢…ê°€']
                if pd.isna(base_price) or base_price == 0:
                    continue

                for horizon, label in [(4, '5d'), (9, '10d'), (14, '15d')]:
                    if len(future) > horizon:
                        future_price = future.iloc[horizon]['ì¢…ê°€']
                        if pd.notna(future_price):
                            ret = (future_price - base_price) / base_price
                            results.append({
                                'stock': code,
                                'date': news_date,
                                'sentiment': row['avg_sentiment'],
                                'news_count': row['news_count'],
                                'horizon': label,
                                'return': ret,
                            })
            time.sleep(0.3)
        except Exception as e:
            print(f"    {code} ì‹¤íŒ¨: {e}")

    if not results:
        print("  âŒ ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, ìƒê´€ê´€ê³„ ë¶„ì„ ê±´ë„ˆëœ€")
        return {}

    res_df = pd.DataFrame(results)
    print(f"\n  ë¶„ì„ ë°ì´í„°í¬ì¸íŠ¸: {len(res_df):,}ê±´")
    print()

    correlations = {}
    for horizon in ['5d', '10d', '15d']:
        subset = res_df[res_df['horizon'] == horizon]
        if len(subset) < 10:
            continue
        corr = subset['sentiment'].corr(subset['return'])
        correlations[horizon] = corr
        print(f"  ê°ì„± vs {horizon} ìˆ˜ìµë¥  ìƒê´€ê³„ìˆ˜: {corr:+.4f} (n={len(subset)})")

    # ê°ì„± êµ¬ê°„ë³„ ìˆ˜ìµë¥ 
    print("\n  ê°ì„± êµ¬ê°„ë³„ í‰ê·  ìˆ˜ìµë¥  (10ì¼):")
    subset_10d = res_df[res_df['horizon'] == '10d']
    if len(subset_10d) > 0:
        bins = [-1.01, -0.3, -0.01, 0.01, 0.3, 1.01]
        labels = ['ê°•ë¶€ì •', 'ì•½ë¶€ì •', 'ì¤‘ë¦½', 'ì•½ê¸ì •', 'ê°•ê¸ì •']
        subset_10d = subset_10d.copy()
        subset_10d['sent_bin'] = pd.cut(subset_10d['sentiment'], bins=bins, labels=labels)
        grouped = subset_10d.groupby('sent_bin', observed=True)['return'].agg(['mean', 'count'])
        for label_name, row in grouped.iterrows():
            arrow = "ğŸ“ˆ" if row['mean'] > 0 else "ğŸ“‰"
            print(f"    {label_name}: {row['mean']:+.2%} (n={int(row['count'])}) {arrow}")

    # ë‰´ìŠ¤ ë³¼ë¥¨ vs ìˆ˜ìµë¥ 
    print("\n  ë‰´ìŠ¤ ë³¼ë¥¨ vs ìˆ˜ìµë¥  (10ì¼):")
    if len(subset_10d) > 0:
        vol_corr = subset_10d['news_count'].corr(subset_10d['return'])
        print(f"    ë‰´ìŠ¤ ê±´ìˆ˜ vs ìˆ˜ìµë¥  ìƒê´€: {vol_corr:+.4f}")

        # ë‰´ìŠ¤ ë§ì€ vs ì ì€
        median_vol = subset_10d['news_count'].median()
        high_vol = subset_10d[subset_10d['news_count'] > median_vol]['return'].mean()
        low_vol = subset_10d[subset_10d['news_count'] <= median_vol]['return'].mean()
        print(f"    ë‰´ìŠ¤ ë§ì€ ì¢…ëª© í‰ê·  ìˆ˜ìµë¥ : {high_vol:+.2%}")
        print(f"    ë‰´ìŠ¤ ì ì€ ì¢…ëª© í‰ê·  ìˆ˜ìµë¥ : {low_vol:+.2%}")

    print()
    if correlations:
        avg_corr = np.mean(list(correlations.values()))
        if avg_corr < 0:
            print(f"  âš ï¸ ì§„ë‹¨: ê°ì„±-ìˆ˜ìµë¥  í‰ê·  ìƒê´€ {avg_corr:+.4f} â†’ ì—­ìƒê´€ (í›„í–‰ ì§€í‘œ ì˜ì‹¬)")
        elif avg_corr < 0.05:
            print(f"  âš ï¸ ì§„ë‹¨: ê°ì„±-ìˆ˜ìµë¥  í‰ê·  ìƒê´€ {avg_corr:+.4f} â†’ ë¬´ì˜ë¯¸ (ì˜ˆì¸¡ë ¥ ì—†ìŒ)")
        else:
            print(f"  âœ… ê°ì„±-ìˆ˜ìµë¥  í‰ê·  ìƒê´€ {avg_corr:+.4f}")

    return correlations


def diagnose_keyword_effectiveness(df):
    """Phase 1-3: ì–´ë–¤ í‚¤ì›Œë“œê°€ ì‹¤ì œ ì£¼ê°€ì™€ ì—°ê´€ë˜ëŠ”ì§€"""
    print("=" * 70)
    print("3. í‚¤ì›Œë“œ íš¨ê³¼ ë¶„ì„")
    print("=" * 70)

    from news_scraper.sentiment_analyzer import SentimentAnalyzer

    pos_kw = SentimentAnalyzer.POSITIVE_KEYWORDS
    neg_kw = SentimentAnalyzer.NEGATIVE_KEYWORDS

    stock_news = df[df['related_stocks'].str.match(r'^\d{6}$', na=False)].copy()

    # ê° í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ì˜ sentiment_score ë¶„í¬
    print("\n  ê¸ì • í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ (ìƒìœ„ 20):")
    pos_counts = {}
    for kw in pos_kw:
        mask = stock_news['title'].str.contains(kw, na=False) | \
               stock_news['content'].fillna('').str.contains(kw, na=False)
        pos_counts[kw] = mask.sum()

    for kw, cnt in sorted(pos_counts.items(), key=lambda x: -x[1])[:20]:
        print(f"    '{kw}': {cnt:,}ê±´")

    print("\n  ë¶€ì • í‚¤ì›Œë“œ ì¶œí˜„ ë¹ˆë„ (ìƒìœ„ 20):")
    neg_counts = {}
    for kw in neg_kw:
        mask = stock_news['title'].str.contains(kw, na=False) | \
               stock_news['content'].fillna('').str.contains(kw, na=False)
        neg_counts[kw] = mask.sum()

    for kw, cnt in sorted(neg_counts.items(), key=lambda x: -x[1])[:20]:
        print(f"    '{kw}': {cnt:,}ê±´")

    # ë¬¸ì œ: ë„ˆë¬´ ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ
    print("\n  âš ï¸ ì ì¬ì  ë¬¸ì œ í‚¤ì›Œë“œ (ë„ˆë¬´ ì¼ë°˜ì ì´ê±°ë‚˜ ëª¨í˜¸):")
    ambiguous = ['ê´€ì‹¬', 'ì£¼ëª©', 'ì¸ê¸°', 'í™”ì œ', 'ì´ìŠˆ', 'ë¬¸ì œ', 'ì‚¬ê±´']
    for kw in ambiguous:
        total = pos_counts.get(kw, 0) + neg_counts.get(kw, 0)
        if total > 100:
            print(f"    '{kw}': {total:,}ê±´ â€” ëª¨í˜¸í•œ í‚¤ì›Œë“œ, ë…¸ì´ì¦ˆ ìœ ë°œ ê°€ëŠ¥")

    return pos_counts, neg_counts


def diagnose_data_quality(df):
    """ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ì§„ë‹¨"""
    print("=" * 70)
    print("4. ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨")
    print("=" * 70)

    # related_stocks í•„ë“œ ë¶„ì„
    has_stocks = df['related_stocks'].notna() & (df['related_stocks'] != '')
    valid_6digit = df['related_stocks'].str.match(r'^\d{6}$', na=False)
    multi_stock = df['related_stocks'].str.contains(',', na=False)

    print(f"  related_stocks ë¹„ì–´ìˆìŒ: {(~has_stocks).sum():,}ê±´ ({(~has_stocks).sum()/len(df)*100:.1f}%)")
    print(f"  ìœ íš¨ 6ìë¦¬ ì¢…ëª©ì½”ë“œ: {valid_6digit.sum():,}ê±´")
    print(f"  ë‹¤ì¤‘ ì¢…ëª©ì½”ë“œ: {multi_stock.sum():,}ê±´")

    # '122025' ë“± ì˜ëª»ëœ ì½”ë“œ
    invalid = has_stocks & ~valid_6digit & ~multi_stock
    print(f"  ë¹„ì •ìƒ ì½”ë“œ: {invalid.sum():,}ê±´")
    if invalid.sum() > 0:
        bad_codes = df[invalid]['related_stocks'].value_counts().head(5)
        for code, cnt in bad_codes.items():
            print(f"    '{code[:20]}...': {cnt:,}ê±´")

    print(f"\n  âš ï¸ ì§„ë‹¨: related_stocks íŒŒì‹± ì˜¤ë¥˜ë¡œ {invalid.sum():,}ê±´ì´ ë¶„ì„ì—ì„œ ëˆ„ë½")
    print()


def diagnose_lagging_indicator(df):
    """Phase 1 ì¶”ê°€: í›„í–‰ ì§€í‘œ ì¦ê±°"""
    print("=" * 70)
    print("5. í›„í–‰ ì§€í‘œ ë¶„ì„")
    print("=" * 70)

    stock_news = df[df['related_stocks'].str.match(r'^\d{6}$', na=False)].copy()

    # ë‰´ìŠ¤ ê°ì„±ì´ ê¸ì •ì¸ ë‚ ê³¼ ë¶€ì •ì¸ ë‚ ì˜ ì§ì „ ì£¼ê°€ ì›€ì§ì„ ë¹„êµ
    print("  (ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... ìƒìœ„ 5 ì¢…ëª©)")
    from news_scraper.price_fetcher import PriceFetcher
    import time

    fetcher = PriceFetcher()
    top5 = stock_news['related_stocks'].value_counts().head(5).index.tolist()

    prior_results = []
    for code in top5:
        try:
            price_df = fetcher.get_daily_price(code, pages=10)
            if price_df.empty:
                continue
            price_df = price_df.sort_values('ë‚ ì§œ').reset_index(drop=True)
            price_df['ë‚ ì§œ'] = pd.to_datetime(price_df['ë‚ ì§œ'])
            price_df['ì¢…ê°€'] = pd.to_numeric(price_df['ì¢…ê°€'].astype(str).str.replace(',', ''), errors='coerce')

            code_daily = stock_news[stock_news['related_stocks'] == code].groupby(
                'published_date'
            )['sentiment_score'].mean().reset_index()

            for _, row in code_daily.iterrows():
                news_date = row['published_date']
                past = price_df[price_df['ë‚ ì§œ'] <= news_date].sort_values('ë‚ ì§œ')
                if len(past) < 6:
                    continue
                # ë‰´ìŠ¤ ë°œí–‰ ì§ì „ 5ì¼ ìˆ˜ìµë¥ 
                prior_ret = (past.iloc[-1]['ì¢…ê°€'] - past.iloc[-6]['ì¢…ê°€']) / past.iloc[-6]['ì¢…ê°€']
                if pd.notna(prior_ret):
                    prior_results.append({
                        'sentiment': row['sentiment_score'],
                        'prior_5d_return': prior_ret,
                    })
            time.sleep(0.3)
        except Exception as e:
            print(f"    {code} ì‹¤íŒ¨: {e}")

    if prior_results:
        prior_df = pd.DataFrame(prior_results)
        corr = prior_df['sentiment'].corr(prior_df['prior_5d_return'])
        print(f"\n  ê°ì„± vs ì§ì „5ì¼ ìˆ˜ìµë¥  ìƒê´€: {corr:+.4f} (n={len(prior_df)})")
        if corr > 0.05:
            print(f"  âš ï¸ ì§„ë‹¨: ì–‘ì˜ ìƒê´€ â†’ ë‰´ìŠ¤ê°€ ì´ë¯¸ ë°œìƒí•œ ì£¼ê°€ ì›€ì§ì„ì„ í›„í–‰ ë°˜ì˜")
            print(f"           (ì£¼ê°€ê°€ ì˜¤ë¥¸ í›„ ê¸ì • ë‰´ìŠ¤ê°€ ë‚˜ì˜¤ëŠ” íŒ¨í„´)")
        elif corr < -0.05:
            print(f"  â„¹ï¸ ì—­ìƒê´€ â†’ ë‰´ìŠ¤ ê°ì„±ì´ ì£¼ê°€ í•˜ë½ í›„ ë“±ì¥ (í‰ê· íšŒê·€?)")
        else:
            print(f"  â„¹ï¸ ìƒê´€ ë¯¸ë¯¸ â†’ í›„í–‰ ì§€í‘œ ì¦ê±° ë¶ˆì¶©ë¶„")
    print()


def main():
    print("NewsQuant ê°ì„±ë¶„ì„ ì§„ë‹¨ ë¦¬í¬íŠ¸")
    print(f"ì‹¤í–‰ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    df = load_news_df()
    print(f"ë¡œë“œëœ ë‰´ìŠ¤: {len(df):,}ê±´ ({df['published_date'].min()} ~ {df['published_date'].max()})")
    print()

    # 1. ë¶„í¬ ë¶„ì„
    dist = diagnose_sentiment_distribution(df)

    # 2. ë°ì´í„° í’ˆì§ˆ
    diagnose_data_quality(df)

    # 3. í‚¤ì›Œë“œ íš¨ê³¼
    kw_pos, kw_neg = diagnose_keyword_effectiveness(df)

    # 4. ìƒê´€ê´€ê³„ (ë„¤íŠ¸ì›Œí¬ í•„ìš”)
    try:
        correlations = diagnose_stock_sentiment_vs_return(df)
    except Exception as e:
        print(f"  ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        correlations = {}

    # 5. í›„í–‰ ì§€í‘œ
    try:
        diagnose_lagging_indicator(df)
    except Exception as e:
        print(f"  í›„í–‰ ì§€í‘œ ë¶„ì„ ì‹¤íŒ¨: {e}")

    # ìš”ì•½
    print("=" * 70)
    print("ì§„ë‹¨ ìš”ì•½")
    print("=" * 70)
    print(f"  1. ì¤‘ë¦½ ë¹„ìœ¨ {dist['zero_ratio']*100:.0f}% â†’ ê°ì„±ë¶„ì„ ë³€ë³„ë ¥ ì‹¬ê°í•˜ê²Œ ë¶€ì¡±")
    print(f"  2. related_stocks íŒŒì‹± ì˜¤ë¥˜ë¡œ ë‹¤ìˆ˜ ë‰´ìŠ¤ê°€ ì¢…ëª© ë§¤í•‘ ì‹¤íŒ¨")
    print(f"  3. í‚¤ì›Œë“œ ì‚¬ì „ì— 'ì´ìŠˆ', 'ê´€ì‹¬' ë“± ëª¨í˜¸í•œ ë‹¨ì–´ í¬í•¨ â†’ ë…¸ì´ì¦ˆ")
    print(f"  4. ê°ì„± ì ìˆ˜ê°€ ì‹¤ì œ ë¯¸ë˜ ìˆ˜ìµë¥ ê³¼ ë¬´ìƒê´€ ë˜ëŠ” ì—­ìƒê´€ â†’ ì˜ˆì¸¡ë ¥ ì—†ìŒ")
    print(f"  5. ê¸ì • ë‰´ìŠ¤ëŠ” ì´ë¯¸ ì£¼ê°€ê°€ ì˜¤ë¥¸ í›„ ë°œí–‰ë˜ëŠ” íŒ¨í„´ (í›„í–‰ ì§€í‘œ)")
    print()
    print("  â†’ docs/sentiment_improvement_plan.md ì°¸ê³ ")


if __name__ == "__main__":
    main()
